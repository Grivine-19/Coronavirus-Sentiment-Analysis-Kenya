{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snscrape.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMF4LwmzMaMD6VFJW+XroZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python38564bitbaseconda378a717c3d1e409e93df1db8692daf27",
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNb80A2ZufQF"
      },
      "source": [
        "**<center>OVERVIEW</center>**\n",
        "\n",
        "> #### Here, I use snscrape as a library that allows us to fetch data without the limitations of tweepy, especially, after twitter rendered GetOldTweets3 obsolete due to change in its privacy policies.\n",
        "\n",
        "> #### Snscrape works only if you have the right version of Python installed. That is, Python 3.8 or higher.\n",
        "\n",
        "> #### At the time of writing this, it is more recommended to use the developer version of snscrape as it has more useful functionality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYa7kFtYxXnP",
        "outputId": "4e3bfb0e-c0e8-4cdd-b200-eebcfac22408"
      },
      "source": [
        "#Download the developer version of snscrape\n",
        "#Do not use \"pip install snscrape\" as it does not give you the developer version of the library.\n",
        "#This version of snscrape cannot install on Google Colab Because colab uses python version 3.6. We require version 3.8 or higher for the task.\n",
        "#The solution is to run the command locally(local runtime environment on computer)\n",
        "\n",
        "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
            "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /tmp/pip-req-build-85rwqher\n",
            "Requirement already satisfied (use --upgrade to upgrade): snscrape==0.3.5.dev95+g5cd3b7d from git+https://github.com/JustAnotherArchivist/snscrape.git in /home/grivine/anaconda3/lib/python3.8/site-packages\n",
            "Requirement already satisfied: requests[socks] in /home/grivine/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev95+g5cd3b7d) (2.24.0)\n",
            "Requirement already satisfied: lxml in /home/grivine/.local/lib/python3.8/site-packages (from snscrape==0.3.5.dev95+g5cd3b7d) (4.5.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/grivine/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev95+g5cd3b7d) (4.9.3)\n",
            "Requirement already satisfied: pytz in /home/grivine/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev95+g5cd3b7d) (2020.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/grivine/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev95+g5cd3b7d) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/grivine/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev95+g5cd3b7d) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/grivine/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev95+g5cd3b7d) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/grivine/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev95+g5cd3b7d) (2020.6.20)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/grivine/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev95+g5cd3b7d) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/grivine/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->snscrape==0.3.5.dev95+g5cd3b7d) (2.0.1)\n",
            "Building wheels for collected packages: snscrape\n",
            "  Building wheel for snscrape (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for snscrape: filename=snscrape-0.3.5.dev95+g5cd3b7d-py3-none-any.whl size=50122 sha256=7cdcadbaaaefcec539d10a8f04eb0bda5021e771c435058cc3b6284d36981c58\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5qv6f5r7/wheels/92/42/87/33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
            "Successfully built snscrape\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#### To get the latitude and longitude of the cities in Kenya, I used this site; [LatLong.net](https://www.latlong.net/category/cities-115-15.html) for the geocode search parameter."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Importing required libraries for the task\n",
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd \n",
        "\n",
        "# Creating list to append tweet data to\n",
        "\n",
        "tweets_list2 = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('UhuruAddress geocode:-1.286389,36.817223,20km, since:2020-02-15 until:2020-07-31').get_items()):\n",
        "    if i>10:\n",
        "        tweets_list2.append([tweet.date, tweet.content, tweet.retweetCount, tweet.likeCount])\n",
        "\n",
        "#Removed: tweet.id, tweet.url, tweet.user.username\n",
        "    \n",
        "#Creating a dataframe from the tweets list above\n",
        "\n",
        "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime',  'Text', 'retweets', 'likes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Datetime  \\\n",
              "0 2020-06-06 19:28:03+00:00   \n",
              "1 2020-06-06 13:25:27+00:00   \n",
              "2 2020-06-06 13:24:27+00:00   \n",
              "3 2020-06-06 13:03:01+00:00   \n",
              "4 2020-06-06 13:02:49+00:00   \n",
              "\n",
              "                                                Text  retweets  likes  \n",
              "0  Mmetukosea sana! \\n\\nSasa ningeenda hadi pahal...         0      2  \n",
              "1  CAN'T WAIT ðŸ¥°ðŸ¥° #6thjune #BlackLivesMatter #Curf...         1     10  \n",
              "2  Wakenya hawachoki na meme za UhuruðŸ˜‚ðŸ˜‚ #UhuruKen...         1      4  \n",
              "3  Fellow Kenyans #uhuruspeaks #UhuruAddress http...         0      1  \n",
              "4  #UhuruKenyatta Cessation of movement in Nairob...         0      2  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Datetime</th>\n      <th>Text</th>\n      <th>retweets</th>\n      <th>likes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-06-06 19:28:03+00:00</td>\n      <td>Mmetukosea sana! \\n\\nSasa ningeenda hadi pahal...</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-06-06 13:25:27+00:00</td>\n      <td>CAN'T WAIT ðŸ¥°ðŸ¥° #6thjune #BlackLivesMatter #Curf...</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-06-06 13:24:27+00:00</td>\n      <td>Wakenya hawachoki na meme za UhuruðŸ˜‚ðŸ˜‚ #UhuruKen...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-06-06 13:03:01+00:00</td>\n      <td>Fellow Kenyans #uhuruspeaks #UhuruAddress http...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-06-06 13:02:49+00:00</td>\n      <td>#UhuruKenyatta Cessation of movement in Nairob...</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "#Load the first few rows of the data frame \n",
        "\n",
        "tweets_df2.head()\n",
        "#tweets_df2.shape"
      ]
    },
    {
      "source": [
        "# <center> **DATA EXTRACTION PHASE** </center>\n",
        "\n",
        "#### Here we store the tweets fetched  in csv format and merge the results from different searches to make a singular dataset for Exploratory Data Analysis."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converting and storing the data in CSV format...:\nLoading the dataset's first few rows:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                   Datetime  \\\n",
              "0           0  2020-06-06 19:28:03+00:00   \n",
              "1           1  2020-06-06 13:25:27+00:00   \n",
              "2           2  2020-06-06 13:24:27+00:00   \n",
              "3           3  2020-06-06 13:03:01+00:00   \n",
              "4           4  2020-06-06 13:02:49+00:00   \n",
              "\n",
              "                                                Text  retweets  likes  \n",
              "0  Mmetukosea sana! \\n\\nSasa ningeenda hadi pahal...         0      2  \n",
              "1  CAN'T WAIT ðŸ¥°ðŸ¥° #6thjune #BlackLivesMatter #Curf...         1     10  \n",
              "2  Wakenya hawachoki na meme za UhuruðŸ˜‚ðŸ˜‚ #UhuruKen...         1      4  \n",
              "3  Fellow Kenyans #uhuruspeaks #UhuruAddress http...         0      1  \n",
              "4  #UhuruKenyatta Cessation of movement in Nairob...         0      2  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Datetime</th>\n      <th>Text</th>\n      <th>retweets</th>\n      <th>likes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2020-06-06 19:28:03+00:00</td>\n      <td>Mmetukosea sana! \\n\\nSasa ningeenda hadi pahal...</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2020-06-06 13:25:27+00:00</td>\n      <td>CAN'T WAIT ðŸ¥°ðŸ¥° #6thjune #BlackLivesMatter #Curf...</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2020-06-06 13:24:27+00:00</td>\n      <td>Wakenya hawachoki na meme za UhuruðŸ˜‚ðŸ˜‚ #UhuruKen...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2020-06-06 13:03:01+00:00</td>\n      <td>Fellow Kenyans #uhuruspeaks #UhuruAddress http...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2020-06-06 13:02:49+00:00</td>\n      <td>#UhuruKenyatta Cessation of movement in Nairob...</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "print(\"converting and storing the data in CSV format...:\")\n",
        "\n",
        "tweets_df2.to_csv('nrb_address.csv')\n",
        "\n",
        "print(\"Loading the dataset's first few rows:\")\n",
        "\n",
        "df1 = pd.read_csv('nrb_address.csv')\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Merging the data from different search phrases\n",
        "\n",
        "data1=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_address.csv')\n",
        "data2=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_curfew.csv')\n",
        "data3=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_ke.csv')\n",
        "data4=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_komesha.csv')\n",
        "data5=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_lockdown.csv')\n",
        "data6=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_safe.csv')\n",
        "data7=pd.read_csv('/home/grivine/Desktop/Get_Old_Tweets_Approach/nrb_tough.csv')\n",
        "\n",
        "nrb_merge_df = pd.concat([data1, data2, data3, data4, data5, data6, data7])\n",
        "\n",
        "#Converting to a csv file\n",
        "\n",
        "nrb_merge_df.to_csv('nairobi_county.csv')\n",
        "\n",
        "nairobi_df = pd.read_csv('nairobi_county.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}